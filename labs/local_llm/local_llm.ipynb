{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just run this cell once to set up your conversation. \n",
    "\n",
    "# Set up your assistant prompt and initialize the message thread.\n",
    "\n",
    "\n",
    "\n",
    "system_prompt = \"\"\"Answer the user's question from the perspective of THE world's leading applicable domain knowledge expert. Process: 1. Rephrase the question \n",
    "to anticipate the user's needs. 2. Break the question into subquestions. 2. Assemble answers in a step-by-step manner organized for optimal comprehension. 3. Include Google Scholar and Google links\n",
    "formatted as follows, that is, do not include direct links to articles, instead search for related terms as follows.:\n",
    "\n",
    "- _See also:_ [Related topics for deeper understanding]\n",
    "  üìö[Research topic articles](https://scholar.google.com/scholar?q=related+terms)\n",
    "  üîç[General information](https://www.google.com/search?q=related+terms)\n",
    "\n",
    "- _You may also enjoy:_ [Topics of tangential interest]\n",
    "  üåü[Explore more](https://www.google.com/search?q=tangential+interest+terms)\n",
    "\n",
    "\"\"\"\n",
    "message_thread = [{\"role\": \"system\", \"content\": system_prompt}]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# add your first and subsequent questions here! Rerun this cell and the next. \n",
    "\n",
    "user_prompt = \"How can I avoid overeating?\"\n",
    "\n",
    "\n",
    "# Define the output file path for the conversation\n",
    "output_file = \"output.md\"\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama\n",
    "\n",
    "# The following function dynamically updates the output file as responses are generated.\n",
    "\n",
    "def clear_output_file():\n",
    "    \"\"\"\n",
    "    Clears all content from the output file by writing an empty string\n",
    "    \"\"\"\n",
    "    with open(output_file, \"w\") as file:\n",
    "        file.write(\"\")  # Clear the file by writing an empty string\n",
    "\n",
    "\n",
    "def write_to_file(content):\n",
    "    \"\"\"\n",
    "    Appends new content to the output file\n",
    "    Args:\n",
    "        content: Text content to append to the file\n",
    "    \"\"\"\n",
    "    with open(output_file, \"a\") as file:\n",
    "        file.write(content)\n",
    "    # If the file is not already open in VSCode, uncomment the next line\n",
    "    # os.system(f\"code {output_file}\"). \n",
    "    # Best, though, to have the file open and in preview mode. Then you see output\n",
    "    # as it is generated!\n",
    "    \n",
    "    # Add the latest prompt to the thread\n",
    "message_thread.append({\"role\": \"user\", \"content\": user_prompt}) \n",
    "\n",
    "\n",
    "# Append the Markdown content to a file\n",
    "with open(output_file, \"a\") as file:\n",
    "    file.write(f'\\n\\n _____ \\n\\n User üë©‚Äç‚öïÔ∏è: {user_prompt} \\n\\n ')\n",
    "\n",
    "llm = Llama(\n",
    "      model_path =\"./Meta-Llama-3-8B-Instruct-Q4_K_S.gguf\",\n",
    "      chat_format = \"llama-3\",\n",
    "      verbose=True,\n",
    "      n_gpu_layers=0, # Uncomment to use GPU acceleration\n",
    "      # seed=1337, # Uncomment to set a specific seed\n",
    "      # n_ctx=2048, # Uncomment to increase the context window\n",
    "  )\n",
    "output = llm.create_chat_completion(\n",
    "        messages = message_thread,\n",
    "        max_tokens=1000,\n",
    "        stream=True,\n",
    "        stop = \"<|assistant|>\",\n",
    "        )\n",
    "\n",
    "text = \"\"\n",
    "\n",
    "for chunk in output:\n",
    "    delta = chunk['choices'][0]['delta']\n",
    "    if 'role' in delta:\n",
    "        role_text = delta['role'] + \"üëΩ\" + \": \"\n",
    "        # print(role_text, end='')\n",
    "        text += role_text\n",
    "        write_to_file(role_text)\n",
    "    elif 'content' in delta:\n",
    "        content_text = delta['content']\n",
    "        # print(content_text, end='')\n",
    "        text += content_text\n",
    "        write_to_file(content_text)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# If the context window is big enough, add the latest response to the thread. You can then go back to update the user prompt and send again.\n",
    "# message_thread.append({\"role\": \"system\", \"content\": text})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or, if you want to clear your output!\n",
    "\n",
    "\n",
    "clear_output_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------\n",
    "# Students will need to fill in the critical functions\n",
    "# Hints are provided within the comments.\n",
    "# -------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Operating system interface - for file and path operations\n",
    "import os\n",
    "\n",
    "# NumPy - Scientific computing library for array operations and numerical computations\n",
    "import numpy as np\n",
    "\n",
    "# PDF processing library - for reading and extracting text from PDF files\n",
    "import PyPDF2\n",
    "\n",
    "# Sentence Transformers - for creating semantic embeddings from text\n",
    "# (Note: This import appears unused in the current code as you're using Llama for embeddings)\n",
    "from sentence_transformers import SentenceTransformer\n",
    "    \n",
    "# Facebook AI Similarity Search (FAISS) - for efficient similarity search and clustering of dense vectors\n",
    "# Used here for storing and querying text embeddings\n",
    "import faiss\n",
    "\n",
    "# Python bindings for the llama.cpp library - provides access to Llama language models\n",
    "# Used for text generation and creating embeddings\n",
    "from llama_cpp import Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the PDF file\n",
    "pdf_file = \"Sample Patient 1.pdf\" \n",
    "\n",
    "# Create a PDF reader object to read the file\n",
    "pdf_reader = PyPDF2.PdfReader(open(pdf_file, \"rb\"))\n",
    "pdf_text = \"\"\n",
    "\n",
    "# Iterate through each page of the PDF\n",
    "for page_num in range(len(pdf_reader.pages)):\n",
    "    # Extract text from the current page\n",
    "    page_text = pdf_reader.pages[page_num].extract_text()\n",
    "    # If text was successfully extracted, add it to our accumulated text\n",
    "    if page_text:\n",
    "        pdf_text += page_text + \"\\n\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, chunk_size=500, chunk_overlap=50) -> list:\n",
    "    \"\"\"\n",
    "    Splits text into overlapping chunks for processing\n",
    "    Args:\n",
    "        text: Input text to be chunked\n",
    "        chunk_size: Number of words per chunk\n",
    "        chunk_overlap: Number of overlapping words between chunks\n",
    "    Returns:\n",
    "        List of text chunks\n",
    "    HINTS:\n",
    "      - Split the input text into a list of words.\n",
    "      - Use a while loop to keep creating chunks until you've processed all words.\n",
    "      - For each chunk:\n",
    "        1. Calculate the end position based on `chunk_size`.\n",
    "        2. Join the words from `start` to `end` into a single string.\n",
    "        3. Append this chunk to a list.\n",
    "        4. Adjust `start` by subtracting `chunk_overlap` to ensure overlapping words.\n",
    "      - Make sure `start` never goes below zero.\n",
    "      - Return the list of chunks.\n",
    "    \"\"\"\n",
    "    #TODO: Implement the function\n",
    "    pass\n",
    "\n",
    "# Process the PDF text into chunks and print the total number of chunks created\n",
    "text_chunks = chunk_text(pdf_text, chunk_size=500, chunk_overlap=50)\n",
    "print(f\"Number of text chunks: {len(text_chunks)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"./Meta-Llama-3-8B-Instruct-Q4_K_S.gguf\"\n",
    "\n",
    "llm_for_embeddings = Llama(\n",
    "    model_path=model_path,    # Path to the Llama model file\n",
    "    n_ctx=2048,              # Context window size\n",
    "    embedding=True,          # Enable embedding generation\n",
    "    verbose=False            # Disable verbose output\n",
    ")\n",
    "\n",
    "print(\"Llama model loaded with embedding support.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def llama_embed_text(text)-> np.ndarray:\n",
    "    \"\"\"\n",
    "    Generates embeddings for input text using Llama model\n",
    "    Args:\n",
    "        text: Input text to embed\n",
    "    Returns:\n",
    "        Numpy array containing the mean embedding vector\n",
    "      HINTS:\n",
    "      1) Generate embeddings for the input text using the Llama model.\n",
    "      2) Use llm_for_embeddings.create_embedding(...)\n",
    "      3) Extract the embedding vectors from the result.\n",
    "      4) Convert the embeddings to a numpy array for numerical operations.\n",
    "      5) Calculate the mean of all token embeddings to get a single vector representation.\n",
    "      6) Return this mean vector as the embedding for the input text.\n",
    "    \"\"\"\n",
    "    #TODO: Implement the function\n",
    "    pass\n",
    "\n",
    "# Process embeddings for all text chunks\n",
    "all_embeddings = []\n",
    "for i, chunk in enumerate(text_chunks):\n",
    "    # TODO: Complete the Loop Code\n",
    "    # 1. Generate embeddings for current chunk using llama_embed_text(chunk)\n",
    "    # 2. Append the resulting embedding vector to all_embeddings list\n",
    "    pass\n",
    "\n",
    "# Stack all embeddings into a single matrix of shape (N, 4096)\n",
    "embeddings_matrix = np.vstack(all_embeddings).astype(\"float32\")\n",
    "\n",
    "# Initialize FAISS index for similarity search\n",
    "index = faiss.IndexFlatL2(4096)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new FAISS index using L2 (Euclidean) distance metric\n",
    "# embeddings_matrix.shape[1] specifies the dimensionality of our vectors (4096 in this case)\n",
    "index = faiss.IndexFlatL2(embeddings_matrix.shape[1])\n",
    "\n",
    "# Add all our document embeddings to the index\n",
    "index.add(embeddings_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_similar_chunks(query, k=3) -> list:\n",
    "    \"\"\"\n",
    "    Searches for text chunks similar to the query using FAISS\n",
    "    Args:\n",
    "        query: Search query text\n",
    "        k: Number of results to return\n",
    "    Returns:\n",
    "        List of tuples containing (text_chunk, distance)\n",
    "    HINTS:\n",
    "      1) Convert 'query' to an embedding.\n",
    "      2) Call index.search(...) to find the top k matches.\n",
    "      3) Return a list of (text_chunk, distance) pairs.\n",
    "      4) Make sure 'query_emb' is reshaped to (1, -1) before searching.\n",
    "      5) Use the indices from index.search to retrieve the chunk texts.\n",
    "    \"\"\"\n",
    "    #TODO: Implement the function\n",
    "    pass\n",
    "\n",
    "# Example usage of the search function\n",
    "test_query = \"How can I avoid overeating?\"\n",
    "search_results = search_similar_chunks(test_query, k=3)\n",
    "\n",
    "# Print each result with its similarity score\n",
    "for i, (chunk, dist) in enumerate(search_results):\n",
    "    print(f\"--- Result {i+1} (distance {dist:.4f}) ---\\n{chunk}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Llama model for chat interactions\n",
    "llm_for_chat = Llama(\n",
    "    model_path=model_path,    # Path to the Llama model weights file\n",
    "    n_ctx=2048,              # Maximum context window size (in tokens)\n",
    "    verbose=True             # Enable detailed logging output\n",
    ")\n",
    "\n",
    "def run_llm(prompt: str) -> str:\n",
    "    \"\"\"\n",
    "    Runs the Llama model with a simple prompt\n",
    "    Args:\n",
    "        prompt: Input prompt text\n",
    "    Returns:\n",
    "        Generated response from the model\n",
    "    HINTS: \n",
    "      1) Create the 'messages' list with system and user roles.\n",
    "      2) Call llm_for_chat.create_chat_completion() with those messages.\n",
    "      3) Return the text from response[\"choices\"][0][\"message\"][\"content\"].\n",
    "    \"\"\"\n",
    "    #TODO: Implement the function\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_llm_with_pdf_knowledge(user_query, k=3):\n",
    "    \"\"\"\n",
    "    Runs the Llama model with context from similar PDF chunks to provide informed answers\n",
    "    \n",
    "    Args:\n",
    "        user_query (str): The user's question to be answered\n",
    "        k (int): Number of similar chunks to retrieve from the PDF (default: 3)\n",
    "    \n",
    "    Returns:\n",
    "        str: Generated response incorporating knowledge from the PDF context\n",
    "    HINTS:\n",
    "      1) Use search_similar_chunks to retrieve top_k chunks.\n",
    "      2) Combine them into a single 'context' string.\n",
    "      3) Construct a prompt that includes the PDF context plus the user query.\n",
    "      4) Pass that prompt to run_llm(prompt).\n",
    "      5) Return the final response text.\n",
    "    \"\"\"\n",
    "    #TODO: Implement the function\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_query = \"Whats is the avg cgm values for my patient?\"\n",
    "# First call: Uses PDF context\n",
    "new_answer = run_llm_with_pdf_knowledge(new_query)\n",
    "print(\"Q:\", new_query)\n",
    "print(\"A:\", new_answer)\n",
    "# Second call: Direct LLM query without context\n",
    "new_answer = run_llm(new_query)\n",
    "print(\"Q:\", new_query)\n",
    "print(\"A:\", new_answer)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
